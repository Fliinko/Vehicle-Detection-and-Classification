{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Computer Vision for AI - Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from imutils.video import VideoStream\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Background Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_background(file):\n",
    "    \"\"\"Extracts background and saves it to a jpg and avi video file\"\"\"\n",
    "\n",
    "    vid = cv2.VideoCapture(file) \n",
    "    #subtractor = cv2.createBackgroundSubtractorMOG2() #Dan isu harira ahjar\n",
    "    subtractor = cv2.createBackgroundSubtractorKNN()\n",
    "\n",
    "    bg_frames = []\n",
    "    i = 0\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "    writer = cv2.VideoWriter('background.avi', fourcc, 30, (800, 600), True)\n",
    "\n",
    "    ret = True\n",
    "    while(ret):\n",
    "        ret, frame = vid.read()\n",
    "\n",
    "        mask = subtractor.apply(frame)\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB)\n",
    "        mask = cv2.bitwise_not(mask) #To change background to white\n",
    "\n",
    "\n",
    "        combined = cv2.bitwise_and(frame, mask)\n",
    "        bg_frames.append(combined)\n",
    "\n",
    "        i+=1\n",
    "        if i == 300:\n",
    "            break\n",
    "\n",
    "    medianFrame = np.median(bg_frames, axis=0).astype(dtype=np.uint8)\n",
    "    cv2.imwrite(\"extracted_background.jpg\", medianFrame)\n",
    "    cv2.imshow(\"Median Frame\", medianFrame)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "    bg_frames = [cv2.addWeighted(frame, 0.1, medianFrame, 0.9, 0) for frame in bg_frames]\n",
    "\n",
    "    for frame in bg_frames:\n",
    "        writer.write(cv2.resize(frame,(800, 600)))\n",
    "\n",
    "\n",
    "    writer.release()\n",
    "    vid.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"\"\n",
    "filename = \"20200323_155250.mp4\"\n",
    "\n",
    "detect_background(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Object Detection with Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trained_object_detection(filename, conf_t=0.5, thresh=0.3, fr_limit=3000):\n",
    "    \n",
    "    labelspath = os.path.join(os.path.dirname(os.getcwd()), \"Trained\", \"obj.names\")\n",
    "    configpath = os.path.join(os.path.dirname(os.getcwd()), \"Trained\", \"yolov4-custom.cfg\")\n",
    "    weightspath = os.path.join(os.path.dirname(os.getcwd()), \"Trained\", \"yolov4-custom_best_5200.weights\")\n",
    "    \n",
    "    LABELS = open(labelspath).read().strip().split(\"\\n\")\n",
    "\n",
    "    COLORS = np.random.uniform(0, 255, size=(len(LABELS), 3))\n",
    "\n",
    "    # load our serialized model from disk\n",
    "    print(\"[INFO] loading model...\")\n",
    "    net = cv2.dnn.readNetFromDarknet(configpath, weightspath)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "    writer = cv2.VideoWriter('output/objectdetection_ours.avi', fourcc, 30, (800, 600), True)\n",
    "\n",
    "    vid = cv2.VideoCapture(filename)\n",
    "    ret = True\n",
    "    \n",
    "    fr_no = 0\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    while(ret):\n",
    "        ret, frame = vid.read()\n",
    "        if not ret:\n",
    "            break\n",
    "                \n",
    "        (H, W) = frame.shape[:2]\n",
    "        # determine only the *output* layer names that we need from YOLO\n",
    "        ln = net.getLayerNames()\n",
    "        ln = [ln[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "        # construct a blob from the input image and then perform a forward\n",
    "        # pass of the YOLO object detector, giving us our bounding boxes and\n",
    "        # associated probabilities\n",
    "        blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "        net.setInput(blob)\n",
    "        layerOutputs = net.forward(ln)\n",
    "\n",
    "        # initialize our lists of detected bounding boxes, confidences, and\n",
    "        # class IDs, respectively\n",
    "        boxes = []\n",
    "        confidences = []\n",
    "        classIDs = []\n",
    "\n",
    "            # loop over each of the layer outputs\n",
    "        for output in layerOutputs:\n",
    "            # loop over each of the detections\n",
    "            for detection in output:\n",
    "                # extract the class ID and confidence (i.e., probability) of\n",
    "                # the current object detection\n",
    "                scores = detection[5:]\n",
    "                classID = np.argmax(scores)\n",
    "                confidence = scores[classID]\n",
    "                # filter out weak predictions by ensuring the detected\n",
    "                # probability is greater than the minimum probability\n",
    "                if confidence > conf_t:\n",
    "                    # scale the bounding box coordinates back relative to the\n",
    "                    # size of the image, keeping in mind that YOLO actually\n",
    "                    # returns the center (x, y)-coordinates of the bounding\n",
    "                    # box followed by the boxes' width and height\n",
    "                    box = detection[0:4] * np.array([W, H, W, H])\n",
    "                    (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "                    # use the center (x, y)-coordinates to derive the top and\n",
    "                    # and left corner of the bounding box\n",
    "                    x = int(centerX - (width / 2))\n",
    "                    y = int(centerY - (height / 2))\n",
    "                    # update our list of bounding box coordinates, confidences,\n",
    "                    # and class IDs\n",
    "                    boxes.append([x, y, int(width), int(height)])\n",
    "                    confidences.append(float(confidence))\n",
    "                    classIDs.append(classID)\n",
    "                    \n",
    "                # apply non-maxima suppression to suppress weak, overlapping bounding\n",
    "        # boxes\n",
    "        idxs = cv2.dnn.NMSBoxes(boxes, confidences, conf_t, thresh)\n",
    "                \n",
    "                # ensure at least one detection exists\n",
    "        if len(idxs) > 0:\n",
    "            # loop over the indexes we are keeping\n",
    "            for i in idxs.flatten():\n",
    "                # extract the bounding box coordinates\n",
    "                (x, y) = (boxes[i][0], boxes[i][1])\n",
    "                (w, h) = (boxes[i][2], boxes[i][3])\n",
    "                # draw a bounding box rectangle and label on the image\n",
    "                color = [int(c) for c in COLORS[classIDs[i]]]\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "                text = \"{}: {:.4f}\".format(LABELS[classIDs[i]], confidences[i])\n",
    "                cv2.putText(frame, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5, color, 2)\n",
    "        \n",
    "        writer.write(cv2.resize(frame,(800, 600)))\n",
    "        \n",
    "        if fr_no >= fr_limit:\n",
    "            break\n",
    "        \n",
    "        if fr_no%200 == 0:\n",
    "            print(fr_no)\n",
    "        \n",
    "        fr_no += 1\n",
    "        \n",
    "    \n",
    "    print(\"[INFO] YOLO took {:.2f} minutes\".format((time.time() - start)/60))\n",
    "    \n",
    "    writer.release()\n",
    "    vid.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"\"\n",
    "filename = \"20200323_155250.mp4\"\n",
    "\n",
    "trained_object_detection(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - Compare Object Detection Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster R-CNN (Deep CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the pretrained model from torchvision.models\n",
    "# Note: pretrained=True will get the pretrained weights for the model.\n",
    "# model.eval() to use the model for inference\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Class labels from official PyTorch documentation for the pretrained model\n",
    "# Note that there are some N/A's\n",
    "# for complete list check https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/\n",
    "# we will use the same list for this notebook\n",
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(img_cv, threshold):\n",
    "    \"\"\"\n",
    "        get_prediction\n",
    "        parameters:\n",
    "        - img_path - path of the input image\n",
    "        - threshold - threshold value for prediction score\n",
    "        method:\n",
    "        - Image is obtained from the image path\n",
    "        - the image is converted to image tensor using PyTorch's Transforms\n",
    "        - image is passed through the model to get the predictions\n",
    "        - class, box coordinates are obtained, but only prediction score > threshold\n",
    "        are chosen.\n",
    "    \"\"\"\n",
    "    transform = T.Compose([T.ToTensor()])\n",
    "    img = transform(img_cv)\n",
    "    pred = model([img])\n",
    "    pred_class = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(pred[0]['labels'].numpy())]\n",
    "    pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().numpy())]\n",
    "    pred_score = list(pred[0]['scores'].detach().numpy())\n",
    "    print(\"iteration\")\n",
    "    pred_t = [pred_score.index(x) for x in pred_score if x > threshold][-1]\n",
    "    pred_boxes = pred_boxes[:pred_t + 1]\n",
    "    pred_class = pred_class[:pred_t + 1]\n",
    "    return pred_boxes, pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faster_object_detection(vid_path, threshold=0.7, rect_th=2, text_size=0.5, text_th=2, fr_limit=500):\n",
    "    \"\"\"\n",
    "    object_detection_api\n",
    "    parameters:\n",
    "        - img_path - path of the input image\n",
    "        - threshold - threshold value for prediction score\n",
    "        - rect_th - thickness of bounding box\n",
    "        - text_size - size of the class label text\n",
    "        - text_th - thichness of the text\n",
    "    method:\n",
    "        - prediction is obtained from get_prediction method\n",
    "        - for each prediction, bounding box is drawn and text is written\n",
    "        with opencv\n",
    "        - the final image is displayed\n",
    "    \"\"\"\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "    writer = cv2.VideoWriter('output/objectdetection_faster_rcnn.avi', fourcc, 30, (800, 600), True)\n",
    "\n",
    "    # image = cv2.imread(args[\"image\"])\n",
    "    vid = cv2.VideoCapture(vid_path)\n",
    "    ret = True\n",
    "\n",
    "    fr_no = 0\n",
    "\n",
    "    start = time.time()\n",
    "    print(\"[INFO] Started video processing...\")\n",
    "\n",
    "    while (ret):\n",
    "\n",
    "        ret, frame = vid.read()\n",
    "        if not ret:\n",
    "            print(\"Error\")\n",
    "\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        boxes, pred_cls = get_prediction(Image.fromarray(frame), threshold)\n",
    "        color_index = set(pred_cls)\n",
    "        COLORS = np.random.uniform(0, 255, size=(len(color_index), 3))\n",
    "\n",
    "        for i in range(len(boxes)):\n",
    "            pt1 = (int(boxes[i][0][0]), int(boxes[i][0][1]))\n",
    "            pt2 = (int(boxes[i][1][0]), int(boxes[i][1][1]))\n",
    "            cv2.rectangle(frame, pt1, pt2, color=list(color_index).index(pred_cls[i]), thickness=rect_th)\n",
    "            cv2.putText(frame, pred_cls[i], pt1, cv2.FONT_HERSHEY_SIMPLEX, text_size,\n",
    "                        list(color_index).index(pred_cls[i]), thickness=text_th)\n",
    "\n",
    "        if fr_no % 100 == 0:\n",
    "            print(fr_no)\n",
    "\n",
    "        if fr_no >= fr_limit:\n",
    "            break\n",
    "\n",
    "        fr_no += 1\n",
    "\n",
    "    print(\"[INFO] FasterRCNN took {:.3f} minutes\".format((time.time() - start) / 60))\n",
    "\n",
    "    writer.release()\n",
    "    vid.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"\"\n",
    "filename = \"20200323_155250.mp4\"\n",
    "\n",
    "faster_object_detection(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YoloV3 (Deep CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_object_detection(filename, conf_t=0.5, thresh=0.3, fr_limit=300):\n",
    "    labelspath = \"YOLO Model/darknet/coco.names\"\n",
    "    configpath = \"YOLO Model/darknet/yolov3-320.cfg\"\n",
    "    weightspath = \"YOLO Model/darknet/yolov3-320.weights\"\n",
    "\n",
    "    LABELS = open(labelspath).read().strip().split(\"\\n\")\n",
    "\n",
    "    COLORS = np.random.uniform(0, 255, size=(len(LABELS), 3))\n",
    "\n",
    "    # load our serialized model from disk\n",
    "    print(\"[INFO] loading model...\")\n",
    "    net = cv2.dnn.readNetFromDarknet(configpath, weightspath)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "    writer = cv2.VideoWriter('output/objectdetection_yolo.avi', fourcc, 30, (800, 600), True)\n",
    "\n",
    "    vid = cv2.VideoCapture(filename)\n",
    "    ret = True\n",
    "\n",
    "    fr_no = 0\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    while (ret):\n",
    "        ret, frame = vid.read()\n",
    "        if not ret:\n",
    "            print(\"Error\")\n",
    "\n",
    "        (H, W) = frame.shape[:2]\n",
    "        # determine only the *output* layer names that we need from YOLO\n",
    "        ln = net.getLayerNames()\n",
    "        ln = [ln[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "        # construct a blob from the input image and then perform a forward\n",
    "        # pass of the YOLO object detector, giving us our bounding boxes and\n",
    "        # associated probabilities\n",
    "        blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "        net.setInput(blob)\n",
    "        layerOutputs = net.forward(ln)\n",
    "\n",
    "        # initialize our lists of detected bounding boxes, confidences, and\n",
    "        # class IDs, respectively\n",
    "        boxes = []\n",
    "        confidences = []\n",
    "        classIDs = []\n",
    "\n",
    "        # loop over each of the layer outputs\n",
    "        for output in layerOutputs:\n",
    "            # loop over each of the detections\n",
    "            for detection in output:\n",
    "                # extract the class ID and confidence (i.e., probability) of\n",
    "                # the current object detection\n",
    "                scores = detection[5:]\n",
    "                classID = np.argmax(scores)\n",
    "                confidence = scores[classID]\n",
    "                # filter out weak predictions by ensuring the detected\n",
    "                # probability is greater than the minimum probability\n",
    "                if confidence > conf_t:\n",
    "                    # scale the bounding box coordinates back relative to the\n",
    "                    # size of the image, keeping in mind that YOLO actually\n",
    "                    # returns the center (x, y)-coordinates of the bounding\n",
    "                    # box followed by the boxes' width and height\n",
    "                    box = detection[0:4] * np.array([W, H, W, H])\n",
    "                    (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "                    # use the center (x, y)-coordinates to derive the top and\n",
    "                    # and left corner of the bounding box\n",
    "                    x = int(centerX - (width / 2))\n",
    "                    y = int(centerY - (height / 2))\n",
    "                    # update our list of bounding box coordinates, confidences,\n",
    "                    # and class IDs\n",
    "                    boxes.append([x, y, int(width), int(height)])\n",
    "\n",
    "                    confidences.append(float(confidence))\n",
    "                    classIDs.append(classID)\n",
    "\n",
    "                # apply non-maxima suppression to suppress weak, overlapping bounding\n",
    "        # boxes\n",
    "        idxs = cv2.dnn.NMSBoxes(boxes, confidences, conf_t, thresh)\n",
    "\n",
    "        # ensure at least one detection exists\n",
    "        if len(idxs) > 0:\n",
    "            # loop over the indexes we are keeping\n",
    "            for i in idxs.flatten():\n",
    "                # extract the bounding box coordinates\n",
    "                (x, y) = (boxes[i][0], boxes[i][1])\n",
    "                (w, h) = (boxes[i][2], boxes[i][3])\n",
    "                # draw a bounding box rectangle and label on the image\n",
    "                color = [int(c) for c in COLORS[classIDs[i]]]\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "                text = \"{}: {:.4f}\".format(LABELS[classIDs[i]], confidences[i])\n",
    "                cv2.putText(frame, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            0.5, color, 2)\n",
    "\n",
    "        writer.write(cv2.resize(frame, (800, 600)))\n",
    "\n",
    "        if fr_no >= fr_limit:\n",
    "            break\n",
    "\n",
    "        fr_no += 1\n",
    "\n",
    "    print(\"[INFO] YOLO took {:.3f} minutes\".format((time.time() - start) / 60))\n",
    "\n",
    "    writer.release()\n",
    "    vid.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"\"\n",
    "filename = \"20200323_155250.mp4\"\n",
    "\n",
    "yolo_object_detection(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HOG-SVM (Machine Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts Image Colour for Formatting\n",
    "def convert_color(img, conv='RGB2YCrCb'):\n",
    "    if conv == 'RGB2YCrCb':\n",
    "        return cv2.cvtColor(img, cv2.COLOR_RGB2YCrCb)\n",
    "    if conv == 'BGR2YCrCb':\n",
    "        return cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)\n",
    "    if conv == 'RGB2LUV':\n",
    "        return cv2.cvtColor(img, cv2.COLOR_RGB2LUV)\n",
    "    if conv == 'RGB2Lab':\n",
    "        return cv2.cvtColor(img, cv2.COLOR_RGB2Lab)\n",
    "    if conv == 'RGB2HSV':\n",
    "        return cv2.cvtColor(img, cv2.COLOR_RGB2HSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOG features of image\n",
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block,\n",
    "                        vis=False, feature_vec=True):\n",
    "    # Call with two outputs if vis==True\n",
    "    if vis == True:\n",
    "        features, hog_image = hog(img, orientations=orient,\n",
    "                                  pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                                  cells_per_block=(cell_per_block, cell_per_block),\n",
    "                                  transform_sqrt=False,\n",
    "                                  visualize=vis, feature_vector=feature_vec)\n",
    "        return features, hog_image\n",
    "    # Otherwise call with one output\n",
    "    else:\n",
    "        features = hog(img, orientations=orient,\n",
    "                       pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                       cells_per_block=(cell_per_block, cell_per_block),\n",
    "                       transform_sqrt=False,\n",
    "                       visualize=vis, feature_vector=feature_vec)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial features of image\n",
    "def bin_spatial(img, size=(32, 32)):\n",
    "    color1 = cv2.resize(img[:,:,0], size).ravel()\n",
    "    color2 = cv2.resize(img[:,:,1], size).ravel()\n",
    "    color3 = cv2.resize(img[:,:,2], size).ravel()\n",
    "    return np.hstack((color1, color2, color3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hist features of image\n",
    "def color_hist(img, nbins=32, hist_range=(0, 256)):\n",
    "    # Compute the histogram of the color channels separately\n",
    "    channel1_hist = np.histogram(img[:,:,0], bins=nbins, range=hist_range)\n",
    "    channel2_hist = np.histogram(img[:,:,1], bins=nbins, range=hist_range)\n",
    "    channel3_hist = np.histogram(img[:,:,2], bins=nbins, range=hist_range)\n",
    "    # Concatenate the histograms into a single feature vector\n",
    "    hist_features = np.concatenate((channel1_hist[0], channel2_hist[0], channel3_hist[0]))\n",
    "    # Return the individual histograms, bin_centers and feature vector\n",
    "    return hist_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract features from a list of images\n",
    "# Have this function call bin_spatial() and color_hist()\n",
    "def extract_features(imgs, color_space='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, orient=9,\n",
    "                        pix_per_cell=8, cell_per_block=2, hog_channel=0,\n",
    "                        spatial_feat=True, hist_feat=True, hog_feat=True, hist_range=(0, 256)):\n",
    "    # Create a list to append feature vectors to\n",
    "    features = []\n",
    "    # Iterate through the list of images\n",
    "    for file in imgs:\n",
    "        file_features = []\n",
    "        # Read in each one by one\n",
    "        image = mpimg.imread(file)\n",
    "        #image *= 255.0\n",
    "        #image = image.astype(np.uint8)\n",
    "\n",
    "        # apply color conversion if other than 'RGB'\n",
    "        if color_space != 'RGB':\n",
    "            if color_space == 'RGB2HSV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "            elif color_space == 'RGB2LUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2LUV)\n",
    "            elif color_space == 'RGB2HLS':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "            elif color_space == 'RGB2YUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n",
    "            elif color_space == 'RGB2YCrCb':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\n",
    "            elif color_space == 'RGB2Lab':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2Lab)\n",
    "        else: feature_image = np.copy(image)\n",
    "\n",
    "        if spatial_feat == True:\n",
    "            spatial_features = bin_spatial(feature_image, size=spatial_size)\n",
    "            file_features.append(spatial_features)\n",
    "        if hist_feat == True:\n",
    "            # Apply color_hist()\n",
    "            hist_features = color_hist(feature_image, nbins=hist_bins, hist_range=hist_range)\n",
    "            file_features.append(hist_features)\n",
    "        if hog_feat == True:\n",
    "        # Call get_hog_features() with vis=False, feature_vec=True\n",
    "            if hog_channel == 'ALL':\n",
    "                hog_features = []\n",
    "                for channel in range(feature_image.shape[2]):\n",
    "                    hog_features.append(get_hog_features(feature_image[:,:,channel],\n",
    "                                        orient, pix_per_cell, cell_per_block,\n",
    "                                        vis=False, feature_vec=True))\n",
    "                hog_features = np.ravel(hog_features)\n",
    "            else:\n",
    "                hog_features = get_hog_features(feature_image[:,:,hog_channel], orient,\n",
    "                            pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "            # Append the new feature vector to the features list\n",
    "            file_features.append(hog_features)\n",
    "        features.append(np.concatenate(file_features))\n",
    "    # Return list of feature vectors\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles = glob.glob('F:/data/vehicles/*.png')\n",
    "nonvehicles = glob.glob('F:/data/non-vehicles/*.png')\n",
    "\n",
    "# print(\"Number of Vehicles in Dataset: \", len(vehicles))\n",
    "# print(\"Number of Non-Vehicles in Dataset:\",len(nonvehicles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVM Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_space = 'RGB2YCrCb' # Can be RGB, RGB2HSV, RGB2LUV, RGB2HLS, RGB2YUV, RGB2YCrCb, RGB2Lab\n",
    "orient = 9  # HOG orientations\n",
    "pix_per_cell = 8 # HOG pixels per cell\n",
    "cell_per_block = 2 # HOG cells per block\n",
    "hog_channel = \"ALL\" # Can be 0, 1, 2, or \"ALL\"\n",
    "spatial_size = (32, 32) # Spatial binning dimensions, rank: 32 >= 16\n",
    "hist_bins = 64    # Number of histogram bins\n",
    "hist_range=(0, 256)\n",
    "spatial_feat = True # Spatial features on or off\n",
    "hist_feat = True # Histogram features on or off\n",
    "hog_feat = True # HOG features on or off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_v = extract_features(vehicles, color_space=color_space,\n",
    "                        spatial_size=spatial_size, hist_bins=hist_bins,\n",
    "                        orient=orient, pix_per_cell=pix_per_cell,\n",
    "                        cell_per_block=cell_per_block,\n",
    "                        hog_channel=hog_channel, spatial_feat=spatial_feat,\n",
    "                        hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "\n",
    "features_nv = extract_features(nonvehicles, color_space=color_space,\n",
    "                        spatial_size=spatial_size, hist_bins=hist_bins,\n",
    "                        orient=orient, pix_per_cell=pix_per_cell,\n",
    "                        cell_per_block=cell_per_block,\n",
    "                        hog_channel=hog_channel, spatial_feat=spatial_feat,\n",
    "                        hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "\n",
    "X = np.vstack((features_v, features_nv)).astype(np.float64)\n",
    "\n",
    "X_scaler = StandardScaler().fit(X)\n",
    "scaled_X = X_scaler.transform(X)\n",
    "\n",
    "# Define the labels vector\n",
    "y = np.hstack((np.ones(len(features_v)), np.zeros(len(features_nv))))\n",
    "\n",
    "\n",
    "# Split up data into randomized training and test sets\n",
    "rand_state = np.random.randint(0, 100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    scaled_X, y, test_size=0.2, random_state=rand_state)\n",
    "\n",
    "print('Using:', orient, 'orientations', pix_per_cell, 'pixels per cell and', cell_per_block, 'cells per block')\n",
    "print('Feature vector length:', len(X_train[0]))\n",
    "\n",
    "svc = LinearSVC()\n",
    "start = time.time()\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "print(round(end-start, 2), 'Seconds to train SVC...')\n",
    "\n",
    "print('Test Accuracy of SVC = ', round(svc.score(X_test, y_test), 4))\n",
    "t = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training for Vehicle Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts features and detects vehicles in a given image.\n",
    "# Returns windows containing possible detections.\n",
    "def find_cars(img, ystart, ystop, scale, svc, X_scaler, orient, pix_per_cell, cell_per_block, spatial_size, hist_bins,\n",
    "              color_space='RGB2YCrCb'):\n",
    "    img_tosearch = img[ystart:ystop, :, :]\n",
    "    ctrans_tosearch = convert_color(img_tosearch, conv=color_space)\n",
    "    if scale != 1:\n",
    "        imshape = ctrans_tosearch.shape\n",
    "        ctrans_tosearch = cv2.resize(ctrans_tosearch, (np.int(imshape[1] / scale), np.int(imshape[0] / scale)))\n",
    "\n",
    "    ch1 = ctrans_tosearch[:, :, 0]\n",
    "    ch2 = ctrans_tosearch[:, :, 1]\n",
    "    ch3 = ctrans_tosearch[:, :, 2]\n",
    "\n",
    "    # Define blocks and steps as above\n",
    "    nxblocks = (ch1.shape[1] // pix_per_cell) - cell_per_block + 1\n",
    "    nyblocks = (ch1.shape[0] // pix_per_cell) - cell_per_block + 1\n",
    "    nfeat_per_block = orient * cell_per_block ** 2\n",
    "\n",
    "    # 64 was the orginal sampling rate, with 8 cells and 8 pix per cell\n",
    "    window = 64\n",
    "    nblocks_per_window = (window // pix_per_cell) - cell_per_block + 1\n",
    "    cells_per_step = 2  # Instead of overlap, define how many cells to step\n",
    "    nxsteps = ((nxblocks - nblocks_per_window) // cells_per_step) + 1\n",
    "    nysteps = ((nyblocks - nblocks_per_window) // cells_per_step) + 1\n",
    "\n",
    "    # Compute individual channel HOG features for the entire image\n",
    "    hog1 = get_hog_features(ch1, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    hog2 = get_hog_features(ch2, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    hog3 = get_hog_features(ch3, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "\n",
    "    hot_windows = []\n",
    "\n",
    "    for xb in range(nxsteps):\n",
    "        for yb in range(nysteps):\n",
    "            ypos = yb * cells_per_step\n",
    "            xpos = xb * cells_per_step\n",
    "            # Extract HOG for this patch\n",
    "            hog_feat1 = hog1[ypos:ypos + nblocks_per_window, xpos:xpos + nblocks_per_window].ravel()\n",
    "            hog_feat2 = hog2[ypos:ypos + nblocks_per_window, xpos:xpos + nblocks_per_window].ravel()\n",
    "            hog_feat3 = hog3[ypos:ypos + nblocks_per_window, xpos:xpos + nblocks_per_window].ravel()\n",
    "            hog_features = np.hstack((hog_feat1, hog_feat2, hog_feat3))\n",
    "\n",
    "            xleft = xpos * pix_per_cell\n",
    "            ytop = ypos * pix_per_cell\n",
    "\n",
    "            # Extract the image patch\n",
    "            subimg = cv2.resize(ctrans_tosearch[ytop:ytop + window, xleft:xleft + window], (64, 64))\n",
    "\n",
    "            # Get color features\n",
    "            spatial_features = bin_spatial(subimg, size=spatial_size)\n",
    "            hist_features = color_hist(subimg, nbins=hist_bins)\n",
    "\n",
    "            # Scale features and make a prediction\n",
    "            test_features = X_scaler.transform(\n",
    "                np.hstack((spatial_features, hist_features, hog_features)).reshape(1, -1))\n",
    "            # test_features = X_scaler.transform(np.hstack((shape_feat, hist_feat)).reshape(1, -1))\n",
    "            test_prediction = svc.predict(test_features)\n",
    "\n",
    "            if test_prediction == 1:\n",
    "                xbox_left = np.int(xleft * scale)\n",
    "                ytop_draw = np.int(ytop * scale)\n",
    "                win_draw = np.int(window * scale)\n",
    "                hot_windows.append(\n",
    "                    ((xbox_left, ytop_draw + ystart), (xbox_left + win_draw, ytop_draw + win_draw + ystart)))\n",
    "\n",
    "    return hot_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to draw bounding boxes\n",
    "def draw_boxes(img, bboxes, color=(0, 0, 255), thick=2):\n",
    "    # Make a copy of the image\n",
    "    imcopy = np.copy(img)\n",
    "    # Iterate through the bounding boxes\n",
    "    for bbox in bboxes:\n",
    "        # Draw a rectangle given bbox coordinates\n",
    "        cv2.rectangle(imcopy, bbox[0], bbox[1], color, thick)\n",
    "    # Return the image copy with boxes drawn\n",
    "    return imcopy\n",
    "\n",
    "\n",
    "def add_heat(heatmap, bbox_list, value=1):\n",
    "    # Iterate through list of bboxes\n",
    "    for box in bbox_list:\n",
    "        # Add += 1 for all pixels inside each bbox\n",
    "        # Assuming each \"box\" takes the form ((x1, y1), (x2, y2))\n",
    "        heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] += value\n",
    "\n",
    "    # Return updated heatmap\n",
    "    return heatmap\n",
    "\n",
    "\n",
    "def apply_threshold(heatmap, threshold):\n",
    "    # Zero out pixels below the threshold\n",
    "    heatmap[heatmap <= threshold] = 0\n",
    "    # Return thresholded map\n",
    "    return heatmap\n",
    "\n",
    "\n",
    "def draw_labeled_bboxes(img, labels):\n",
    "    # Iterate through all detected cars\n",
    "    for car_number in range(1, labels[1] + 1):\n",
    "        # Find pixels with each car_number label value\n",
    "        nonzero = (labels[0] == car_number).nonzero()\n",
    "        # Identify x and y values of those pixels\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        # Define a bounding box based on min/max x and y\n",
    "        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "        # Draw the box on the image\n",
    "        cv2.rectangle(img, bbox[0], bbox[1], (0, 0, 255), 6)\n",
    "        cv2.putText(img, 'Vehicles Detected: '+ str(car_number), (50,50), cv2.FONT_HERSHEY_COMPLEX, 0.5, color=(255,0,0), thickness=2)\n",
    "    # Return the image\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ystart = (360, 360, 360, 360)\n",
    "all_ystop = (660, 660, 660, 660)\n",
    "all_scales = (0.9, 1.1, 1.4, 1.7)\n",
    "\n",
    "original_image = mpimg.imread('test.jpg')\n",
    "image = original_image.astype(np.float32)/255.0\n",
    "\n",
    "all_hot_windows = []\n",
    "for ystart, ystop, scale in zip(all_ystart, all_ystop, all_scales):\n",
    "    all_hot_windows += find_cars(image, ystart, ystop, scale, svc, \n",
    "                                 X_scaler, orient, pix_per_cell, \n",
    "                                 cell_per_block, spatial_size, \n",
    "                                 hist_bins, color_space)\n",
    "\n",
    "out_image = draw_boxes(original_image, all_hot_windows)\n",
    "\n",
    "heat = np.zeros_like(original_image[:,:,0]).astype(np.float)\n",
    "# Add heat to each box in box list\n",
    "heat = add_heat(heat, all_hot_windows)\n",
    "# Apply threshold to help remove false positives\n",
    "heat = apply_threshold(heat, 1)\n",
    "# Visualize the heatmap when displaying    \n",
    "heatmap = np.clip(heat, 0, 255)\n",
    "# Find final boxes from heatmap using label function\n",
    "labels = label(heatmap)\n",
    "draw_img = draw_labeled_bboxes(np.copy(original_image), labels)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\n",
    "ax1.imshow(out_image)\n",
    "ax1.set_title('Car Positions for test.jpg', fontsize=16)\n",
    "ax2.imshow(heatmap, cmap='hot')\n",
    "ax2.set_title('Heat Map', fontsize=16)\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Video Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(original_image, heat_thresh=1, heat_queue = deque(maxlen=25), frlimit = 500):\n",
    "    \n",
    "    # Scale image from 0 to 1\n",
    "    image = original_image.astype(np.float32)/255.0\n",
    "\n",
    "        # Perform HOG subsampling as multiple scales\n",
    "    all_ystart = (360, 360, 360, 360)\n",
    "    all_ystop = (660, 660, 660, 660)\n",
    "    all_scales = (0.9, 1.1, 1.4, 1.7)\n",
    "    all_hot_windows = []\n",
    "    for ystart, ystop, scale in zip(all_ystart, all_ystop, all_scales):\n",
    "        all_hot_windows += find_cars(image, ystart, ystop, scale, svc, \n",
    "                                         X_scaler, orient, pix_per_cell, \n",
    "                                         cell_per_block, spatial_size, \n",
    "                                         hist_bins, color_space)\n",
    "\n",
    "\n",
    "    heat = np.zeros_like(original_image[:,:,0]).astype(np.float)\n",
    "        # Add heat to each box in box list\n",
    "    heat = add_heat(heat, all_hot_windows)\n",
    "        #print('heat max 1:', np.max(heat))\n",
    "\n",
    "        # Add heat to circular buffer and find total\n",
    "    heat_queue.append(heat)\n",
    "    total_heat = np.sum(heat_queue, axis=0).astype(np.uint8)\n",
    "        # Apply threshold to help remove false positives\n",
    "    total_heat = apply_threshold(total_heat, heat_thresh)   \n",
    "    heatmap = np.clip(total_heat, 0, 255)\n",
    "        # Find final boxes from heatmap using label function\n",
    "    labels = label(heatmap)\n",
    "\n",
    "    out_image = draw_boxes(original_image, all_hot_windows)\n",
    "    \n",
    "    return draw_labeled_bboxes(np.copy(out_image), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video = 'E:/vehicle-detection-classification-opencv/CV Vids/20200327_160440.mp4'\n",
    "output_video = 'sunny_hog.mp4'\n",
    "clip = VideoFileClip(input_video)\n",
    "processed = clip.fl_image(process_frame)\n",
    "%time processed.write_videofile(output_video, audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 - Car Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_car_counting(filename, conf_t=0.5, thresh=0.3, fr_limit=300):\n",
    "    labelspath = \"YOLO Model/darknet/coco.names\"\n",
    "    configpath = \"YOLO Model/darknet/yolov3-320.cfg\"\n",
    "    weightspath = \"YOLO Model/darknet/yolov3-320.weights\"\n",
    "\n",
    "    LABELS = open(labelspath).read().strip().split(\"\\n\")\n",
    "\n",
    "    COLORS = np.random.uniform(0, 255, size=(len(LABELS), 3))\n",
    "\n",
    "    # load our serialized model from disk\n",
    "    print(\"[INFO] loading model...\")\n",
    "    net = cv2.dnn.readNetFromDarknet(configpath, weightspath)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "    writer = cv2.VideoWriter('output/objectdetection_yolo.avi', fourcc, 30, (800, 600), True)\n",
    "\n",
    "    vid = cv2.VideoCapture(filename)\n",
    "    ret = True\n",
    "\n",
    "    fr_no = 0\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    while (ret):\n",
    "        ret, frame = vid.read()\n",
    "        if not ret:\n",
    "            print(\"Error\")\n",
    "\n",
    "        (H, W) = frame.shape[:2]\n",
    "        # determine only the *output* layer names that we need from YOLO\n",
    "        ln = net.getLayerNames()\n",
    "        ln = [ln[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "        # construct a blob from the input image and then perform a forward\n",
    "        # pass of the YOLO object detector, giving us our bounding boxes and\n",
    "        # associated probabilities\n",
    "        blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "        net.setInput(blob)\n",
    "        layerOutputs = net.forward(ln)\n",
    "\n",
    "        # initialize our lists of detected bounding boxes, confidences, and\n",
    "        # class IDs, respectively\n",
    "        boxes = []\n",
    "        confidences = []\n",
    "        classIDs = []\n",
    "\n",
    "        # loop over each of the layer outputs\n",
    "        for output in layerOutputs:\n",
    "            # loop over each of the detections\n",
    "            for detection in output:\n",
    "                # extract the class ID and confidence (i.e., probability) of\n",
    "                # the current object detection\n",
    "                scores = detection[5:]\n",
    "                classID = np.argmax(scores)\n",
    "                confidence = scores[classID]\n",
    "                # filter out weak predictions by ensuring the detected\n",
    "                # probability is greater than the minimum probability\n",
    "                if confidence > conf_t:\n",
    "                    # scale the bounding box coordinates back relative to the\n",
    "                    # size of the image, keeping in mind that YOLO actually\n",
    "                    # returns the center (x, y)-coordinates of the bounding\n",
    "                    # box followed by the boxes' width and height\n",
    "                    box = detection[0:4] * np.array([W, H, W, H])\n",
    "                    (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "                    # use the center (x, y)-coordinates to derive the top and\n",
    "                    # and left corner of the bounding box\n",
    "                    x = int(centerX - (width / 2))\n",
    "                    y = int(centerY - (height / 2))\n",
    "                    # update our list of bounding box coordinates, confidences,\n",
    "                    # and class IDs\n",
    "                    boxes.append([x, y, int(width), int(height)])\n",
    "\n",
    "                    confidences.append(float(confidence))\n",
    "                    classIDs.append(classID)\n",
    "\n",
    "                # apply non-maxima suppression to suppress weak, overlapping bounding\n",
    "        # boxes\n",
    "        idxs = cv2.dnn.NMSBoxes(boxes, confidences, conf_t, thresh)\n",
    "\n",
    "        # ensure at least one detection exists\n",
    "        if len(idxs) > 0:\n",
    "            counter = len(idxs)\n",
    "            cv2.putText(frame, 'Vehicles Detected: '+ str(counter), (50,50), cv2.FONT_HERSHEY_COMPLEX, text_size, color=(255,0,0), thickness=text_th)\n",
    "\n",
    "\n",
    "        writer.write(cv2.resize(frame, (800, 600)))\n",
    "\n",
    "        if fr_no >= fr_limit:\n",
    "            break\n",
    "\n",
    "        fr_no += 1\n",
    "\n",
    "    print(\"[INFO] YOLO took {:.3f} minutes\".format((time.time() - start) / 60))\n",
    "\n",
    "    writer.release()\n",
    "    vid.release()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6ce3b824504692ac02849902c8b8a9757c2b44c834f8b88396ffb8e175c7b5c1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
